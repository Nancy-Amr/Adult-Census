# Project: Adult Income Prediction Dataset
# Description: Complete data cleaning and preprocessing pipeline for income prediction
# =============================================================================

# =============================================================================
# 1. INSTALL AND LOAD REQUIRED PACKAGES
# =============================================================================

# Install packages if not already installed
if (!require(tidyverse)) install.packages("tidyverse")
if (!require(caret)) install.packages("caret")
if (!require(naniar)) install.packages("naniar")

# Load libraries
library(tidyverse)   # For data manipulation and visualization
library(caret)       # For machine learning utilities and preprocessing
library(naniar)      # For missing value visualization

cat("=== PACKAGES LOADED SUCCESSFULLY ===\n")

# =============================================================================
# 2. LOAD AND INITIAL INSPECTION OF RAW DATA
# =============================================================================

# Load the dataset from CSV file
df <- read_csv("adult.csv")

cat("=== INITIAL DATA INSPECTION ===\n")

# Display dataset structure: shows column names, types, and sample data
cat("\n1. Dataset Structure:\n")
str(df)

# Generate statistical summary: min, max, mean, quartiles for numerical variables
cat("\n2. Statistical Summary:\n")
print(summary(df))

# =============================================================================
# 3. CLEAN COLUMN NAMES FOR CONSISTENCY
# =============================================================================

cat("\n=== CLEANING COLUMN NAMES ===\n")

# Clean column names: convert to lowercase, remove special characters, replace dots with underscores
df <- df %>%
  rename_with(~tolower(gsub("[^[:alnum:]_\\.]", "", .))) %>%  # Remove special chars
  rename_with(~gsub("\\.", "_", .))  # Replace dots with underscores

cat("Column names after cleaning:\n")
print(names(df))

# =============================================================================
# 4. HANDLE MISSING VALUES (CONVERT "?" TO NA)
# =============================================================================

cat("\n=== HANDLING MISSING VALUES ===\n")

# Convert "?" to NA (proper missing value representation) for all character columns
df_clean <- df %>%
  mutate(across(where(is.character), ~na_if(., "?")))

cat("Converted '?' to NA for proper missing value handling.\n")

# =============================================================================
# 5. ANALYZE MISSING VALUES
# =============================================================================

# Calculate percentage of missing values for each variable
missing_summary <- df_clean %>%
  summarise(across(everything(), ~sum(is.na(.))/n())) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "missing_pct")

cat("\nMissing Values Analysis:\n")
print(missing_summary)

# =============================================================================
# 6. IMPUTE MISSING VALUES
# =============================================================================

cat("\n=== IMPUTING MISSING VALUES ===\n")

# Impute missing values using mode (most frequent value) for categorical variables
df_clean <- df_clean %>%
  mutate(
    # For workclass: replace NA with most common workclass (mode imputation)
    workclass = ifelse(is.na(workclass), 
                       names(sort(table(workclass[!is.na(workclass)]), decreasing = TRUE))[1],
                       workclass),
    
    # For occupation: replace NA with most common occupation (mode imputation)
    occupation = ifelse(is.na(occupation),
                        names(sort(table(occupation[!is.na(occupation)]), decreasing = TRUE))[1],
                        occupation),
    
    # For native_country: replace NA with "United-States" (majority class imputation)
    native_country = ifelse(is.na(native_country), "United-States", native_country)
  )

cat("Missing values imputed using mode imputation for categorical variables.\n")

# Verify no missing values remain
cat("\nMissing values after imputation:\n")
print(df_clean %>% summarise(across(everything(), ~sum(is.na(.)))))

# =============================================================================
# 7. OUTLIER DETECTION AND HANDLING
# =============================================================================

cat("\n=== HANDLING OUTLIERS ===\n")

# Define numerical variables for outlier analysis (using CORRECT column names)
numerical_vars <- c("age", "fnlwgt", "capital_gain", "capital_loss", "hours_per_week")

# Function to detect outliers using IQR method
detect_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR
  sum(x < lower | x > upper, na.rm = TRUE)
}

# Count outliers for each numerical variable
outliers_summary <- df_clean %>%
  summarise(across(all_of(numerical_vars), detect_outliers))

cat("\nOutlier Counts (using IQR method):\n")
print(outliers_summary)

# Cap extreme values (winsorization) instead of removal to preserve data
df_clean <- df_clean %>%
  mutate(
    age = ifelse(age > 85, 85, age),                     # Cap age at 85 (99th percentile)
    hours_per_week = ifelse(hours_per_week > 80, 80, hours_per_week),  # Cap hours at 80
    capital_gain_log = log1p(capital_gain),             # Log transform for highly skewed data
    capital_loss_log = log1p(capital_loss)              # Log transform for highly skewed data
  )

cat("Extreme values capped and log transformations applied for skewed variables.\n")

# =============================================================================
# 8. TARGET VARIABLE ANALYSIS
# =============================================================================

cat("\n=== ANALYZING TARGET VARIABLE ===\n")

# Check class distribution in target variable (income)
income_distribution <- df_clean %>%
  count(income) %>%
  mutate(percentage = n/sum(n) * 100)

cat("\nIncome Class Distribution:\n")
print(income_distribution)

# =============================================================================
# 9. ENCODE CATEGORICAL VARIABLES
# =============================================================================

cat("\n=== ENCODING CATEGORICAL VARIABLES ===\n")

# Convert categorical variables to factors with explicit levels
df_clean <- df_clean %>%
  mutate(
    # Convert target variable to ordered factor
    income = factor(income, levels = c("<=50K", ">50K")),
    
    # Convert all categorical variables to factors
    across(c(workclass, education, marital_status, occupation,
             relationship, race, sex, native_country), as.factor)
  )

cat("Categorical variables converted to factors for machine learning compatibility.\n")

# =============================================================================
# 10. FEATURE ENGINEERING
# =============================================================================

cat("\n=== CREATING NEW FEATURES ===\n")

# Create new derived features to improve model performance
df_clean <- df_clean %>%
  mutate(
    # Create age groups for better pattern recognition
    age_group = cut(age, 
                    breaks = c(0, 25, 35, 45, 55, 65, 100),
                    labels = c("18-25", "26-35", "36-45", "46-55", "56-65", "65+"),
                    include.lowest = TRUE),
    
    # Categorize working hours
    work_intensity = cut(hours_per_week,
                         breaks = c(0, 20, 40, 60, 100),
                         labels = c("Part-time", "Full-time", "Overtime", "Excessive"),
                         include.lowest = TRUE),
    
    # Group education levels into meaningful categories
    education_level = case_when(
      education %in% c("Preschool", "1st-4th", "5th-6th", "7th-8th") ~ "Elementary",
      education %in% c("9th", "10th", "11th", "12th", "HS-grad") ~ "High School",
      education %in% c("Some-college", "Assoc-acdm", "Assoc-voc") ~ "Some College",
      education %in% c("Bachelors", "Prof-school") ~ "Bachelor",
      education %in% c("Masters", "Doctorate") ~ "Advanced",
      TRUE ~ "Other"
    ),
    
    # Create binary indicator for capital activity
    has_capital_change = ifelse(capital_gain > 0 | capital_loss > 0, 1, 0),
    
    # Create net wealth indicator
    wealth_indicator = capital_gain - capital_loss
  )

cat("New features created: age groups, work intensity, education levels, capital indicators.\n")

# =============================================================================
# 11. REDUCE CATEGORICAL VARIABLE CARDINALITY
# =============================================================================

cat("\n=== REDUCING CATEGORICAL CARDINALITY ===\n")

# Count frequency of each country
country_counts <- df_clean %>%
  count(native_country, sort = TRUE)

# Identify countries with sufficient representation (more than 100 occurrences)
top_countries <- country_counts %>%
  filter(n > 100) %>%
  pull(native_country)

# Group rare countries as "Other" to reduce dimensionality
df_clean <- df_clean %>%
  mutate(
    native_country_simplified = ifelse(as.character(native_country) %in% top_countries,
                                       as.character(native_country),
                                       "Other"),
    native_country_simplified = as.factor(native_country_simplified)
  )

cat("Reduced native_country categories from", length(levels(df_clean$native_country)),
    "to", length(unique(df_clean$native_country_simplified)), "categories.\n")

# =============================================================================
# 12. FEATURE SCALING/NORMALIZATION
# =============================================================================

cat("\n=== SCALING NUMERICAL FEATURES ===\n")

# Standardize numerical features (mean=0, sd=1) for algorithms sensitive to scale
cat("Numerical variables being scaled:\n")
print(numerical_vars)

preprocess_params <- preProcess(df_clean %>% select(all_of(numerical_vars)),
                                method = c("center", "scale"))

# Apply scaling to create normalized dataset
df_normalized <- predict(preprocess_params, df_clean)

cat("Numerical features standardized (mean=0, sd=1).\n")

# =============================================================================
# 13. DATA SPLITTING FOR MACHINE LEARNING
# =============================================================================

cat("\n=== SPLITTING DATA FOR MACHINE LEARNING ===\n")

# Set random seed for reproducibility
set.seed(123)

# Initial split: 70% training, 30% testing (stratified by income to preserve distribution)
train_index <- createDataPartition(df_normalized$income, 
                                   p = 0.7, 
                                   list = FALSE)
train_data <- df_normalized[train_index, ]
test_data <- df_normalized[-train_index, ]

# Further split training data: 80% final training, 20% validation
val_index <- createDataPartition(train_data$income,
                                 p = 0.2,
                                 list = FALSE)
val_data <- train_data[val_index, ]
train_final <- train_data[-val_index, ]

# =============================================================================
# 14. FINAL DATASET VERIFICATION
# =============================================================================

cat("\n=== FINAL DATASET VERIFICATION ===\n")

# Print dataset dimensions
cat("\nDataset Dimensions:\n")
cat("Full normalized dataset:", dim(df_normalized), "\n")
cat("Training set:", dim(train_final), "\n")
cat("Validation set:", dim(val_data), "\n")
cat("Test set:", dim(test_data), "\n")

# Verify class distribution is preserved across splits
cat("\nClass Distribution Across Splits:\n")
distribution_table <- rbind(
  "Full" = prop.table(table(df_normalized$income)),
  "Train" = prop.table(table(train_final$income)),
  "Validation" = prop.table(table(val_data$income)),
  "Test" = prop.table(table(test_data$income))
)
print(round(distribution_table, 4))

# =============================================================================
# 15. SAVE PROCESSED DATASETS
# =============================================================================

cat("\n=== SAVING PROCESSED DATASETS ===\n")

# Save all processed datasets for future use
write.csv(df_normalized, "adult_income_cleaned.csv", row.names = FALSE)
write.csv(train_final, "adult_income_train.csv", row.names = FALSE)
write.csv(val_data, "adult_income_validation.csv", row.names = FALSE)
write.csv(test_data, "adult_income_test.csv", row.names = FALSE)

cat("‚úì adult_income_cleaned.csv - Complete cleaned dataset\n")
cat("‚úì adult_income_train.csv - Training set (56% of data)\n")
cat("‚úì adult_income_validation.csv - Validation set (14% of data)\n")
cat("‚úì adult_income_test.csv - Test set (30% of data)\n")

# =============================================================================
# 16. CREATE DATA DICTIONARY FOR DOCUMENTATION
# =============================================================================

cat("\n=== CREATING DATA DICTIONARY ===\n")

# Generate comprehensive documentation of all variables
data_dictionary <- data.frame(
  Variable = names(df_normalized),
  Type = sapply(df_normalized, class),
  Description = c(
    "Age of individual",
    "Type of employment",
    "Final weight (sampling weight for population representation)",
    "Highest education level",
    "Education numerical encoding (higher = more education)",
    "Marital status",
    "Occupation category",
    "Relationship status in household",
    "Race",
    "Sex",
    "Capital gains (investment profits)",
    "Capital losses (investment losses)",
    "Hours worked per week",
    "Country of origin",
    "Target variable: Income level (<=50K or >50K)",
    "Log-transformed capital gain (reduces skewness)",
    "Log-transformed capital loss (reduces skewness)",
    "Age group categorization for pattern analysis",
    "Work intensity category based on weekly hours",
    "Education level grouping for simplified analysis",
    "Binary indicator: 1 if any capital change, 0 otherwise",
    "Net wealth indicator (gains minus losses)",
    "Simplified country categorization (frequent countries kept, others grouped)"
  ),
  Missing_Values = sapply(df_normalized, function(x) sum(is.na(x))),
  Unique_Values = sapply(df_normalized, function(x) length(unique(x)))
)

# Save data dictionary
write.csv(data_dictionary, "data_dictionary.csv", row.names = FALSE)
cat("‚úì data_dictionary.csv - Complete variable documentation\n")

# =============================================================================
# 17. FINAL STATUS AND NEXT STEPS
# =============================================================================

# Alternative using paste()
cat(paste0(
  "\n", strrep("=", 60), "\n",
  "‚úÖ DATA PREPROCESSING COMPLETED SUCCESSFULLY!\n",
  strrep("=", 60), "\n\n",
  
  "üìä SUMMARY OF PROCESSING STEPS:\n",
  "1. Loaded raw data (32,561 rows √ó 15 columns)\n",
  "2. Cleaned column names (dots ‚Üí underscores)\n",
  "3. Converted '?' to NA for proper missing value handling\n",
  "4. Imputed missing values (workclass: 5.6%, occupation: 5.7%)\n",
  "5. Detected and capped outliers in age and hours\n",
  "6. Applied log transformation to skewed capital variables\n",
  "7. Encoded categorical variables as factors\n",
  "8. Created 6 new engineered features\n",
  "9. Reduced native_country from 42 to manageable categories\n",
  "10. Standardized numerical features (mean=0, sd=1)\n",
  "11. Split data into Train (56%), Validation (14%), Test (30%)\n",
  "12. Saved 4 processed datasets and data dictionary\n\n",
  
  "üéØ NEXT STEPS FOR YOUR PROJECT:\n",
  "1. Perform Exploratory Data Analysis (EDA)\n",
  "2. Formulate and test hypotheses\n",
  "3. Build machine learning models (Logistic Regression, Random Forest, etc.)\n",
  "4. Evaluate model performance using validation and test sets\n",
  "5. Create project documentation and poster\n\n",
  
  "üìÅ FILES CREATED:\n",
  "‚Ä¢ adult_income_cleaned.csv - Complete processed dataset\n",
  "‚Ä¢ adult_income_train.csv - Training data for model development\n",
  "‚Ä¢ adult_income_validation.csv - Validation data for tuning\n",
  "‚Ä¢ adult_income_test.csv - Test data for final evaluation\n",
  "‚Ä¢ data_dictionary.csv - Complete variable documentation\n\n",
  
  strrep("=", 60), "\n"
))

